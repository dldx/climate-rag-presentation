<script lang="ts">
	let { stepNo }: { stepNo: number } = $props()

	import RagPipeline from '$lib/rag-pipeline.svg?raw'
	import { onMount } from 'svelte'
	import { draggable } from '@neodrag/svelte'
	import { Transition } from '@animotion/core'

	let currentIndex = $state(0)
	const codeSnippets = [
		'What is the FiT tariff won by the Taqa Arabia solar plant in Egypt?',
		'What is the total electricity generated by Kusile Power Station in each month of 2023? Return the result as a table with columns Power station name, Date (Month-Year), Generation (GWh).',
		'What climate risks did AES identify in its 10-k?',
		'Give me all financing details for SECI Benban 2 Arinna by Arinna Solar Power: 20 MW in Egypt'
	]
	let currentExample = $derived(codeSnippets[currentIndex])

	onMount(() => {
		const interval = setInterval(() => {
			currentIndex = (currentIndex + 1) % codeSnippets.length
		}, 3000)

		return () => clearInterval(interval)
	})
</script>

<div
	class="ml-20"
	style="--ask-opacity:{stepNo > 0 || stepNo == 0 ? '1' : '0.1'};
		   --initial-web-search-opacity:{stepNo > 1 || stepNo == 0 ? '1' : '0.1'};
		   --web-search-opacity:{stepNo > 2 || stepNo == 0 ? '1' : '0.1'};
           --scrape-opacity:{stepNo > 3 || stepNo == 0 ? '1' : '0.1'};
		   --ingest-opacity:{stepNo > 4 || stepNo == 0 ? '1' : '0.1'};
		   --retrieve-opacity:{stepNo > 5 || stepNo == 0 ? '1' : '0.1'};
		  --generate-opacity:{stepNo > 6 || stepNo == 0 ? '1' : '0.1'};
		 --feedback-opacity:{stepNo > 7 || stepNo == 0 ? '1' : '0.1'};
		 --answer-opacity:{stepNo > 8 || stepNo == 0 ? '1' : '0.1'};
		   "
>
	{@html RagPipeline}
</div>
{#if stepNo > 0}
<div class="absolute top-0 left-0 flex h-screen w-screen justify-center items-center">
	<div
		use:draggable={{ bounds: 'body', cancel: '.cancel' }}
		class="backdrop-blur absolute left-10 flex max-h-[80%] max-w-[35ch] cursor-grab flex-col gap-4 overflow-auto border-2 p-4 text-left text-xl"
		style="transition: unset !important; border-color: var(--r-heading-color); box-shadow: 5px 5px 0px 0px rgba(37, 178, 100, 0.3); background-color: rgba(20,20,20,0.9);"
	>
		<div class="cancel cursor-auto">
			{#if stepNo == 1}
				<ul class=" list-none pl-5">
                    <li>
                        An example question could be:
                        <p class="block mt-5 border-l-[var(--r-heading-color)] border-l pl-5"><code class="text-lg">{currentExample}</code></p>
                    </li>
                </ul>
			{/if}
			{#if stepNo == 2}
				<ul class="list-none pl-5">
                    <li>
                        Decide to search the web depending on whether or not enough information is in the database
                        to answer the question.
                    </li>
                </ul>
			{/if}
			{#if stepNo == 3}
				<ul class="list-none pl-5">
					<li>Use an LLM to generate multiple search queries in a language of our choice.</li>
					<li>
						These queries are then sent to a search engine:
						<ul class="list-disc pl-5">
							<li>Baidu in the case of Chinese queries.</li>
							<li>Google otherwise.</li>
						</ul>
					</li>
				</ul>
			{/if}
			{#if stepNo == 4}
				<ul class="list-disc pl-5">
					<li>Scrape web pages or PDFs</li>
                    <li>Use a cheap LLM to decide whether to continue crawling.</li>
					<li>Scraping web pages is complex due to dynamic content requiring JavaScript.</li>
					<li>Use a headless browser to render and extract content.</li>
					<li>Employ techniques to avoid being blocked by sites.</li>
					<li>Fallback options include <a href="https://jina.ai" target="_blank">Jina</a> or <a href="https://www.firecrawl.dev" target="_blank">Firecrawl</a> if primary methods fail.</li>
					<li>Converting PDFs to markdown is challenging due to scanned or unreadable documents, or complex tables.</li>
					<li>Use <a href="https://blog.google/technology/ai/google-gemini-ai/" target="_blank">Google Gemini Flash</a> to convert scanned PDFs to text.</li>
				</ul>
			{/if}
			{#if stepNo == 5}
            <ul class="list-disc pl-5">
                <li>Next, divide each document into semantic chunks</li>
                <li><a target="_blank" href="https://youtu.be/8OJC21T2SL4?si=XDXgIZrgao-euzoA">Many ways</a> to chunk a document were tested, each with pros and cons.</li>
                <li>Semantic chunking aims to aggregate contiguous chunks with similar topics</li>
                <li>Store these chunks in a vector database so that they can be retrieved based on semantic similarity, rather than exact matches.</li>
        </ul>
			{/if}
			{#if stepNo == 6}
            <ul class="list-disc pl-5">
                <li>
                    Pre-filter potential documents using fast full-text search with Redis to improve reliability.
                </li>
                <li>
                    Generate a number of queries (or reuse web search queries) to retrieve the most relevant chunks of text.
                </li>
                <li>
                    For each chunk source, generate useful metadata, such as the source's title, author, publication date, key entities, and more.
                </li>
                <li>
                    Pass this augmented set of chunks to a reranking model to select only most relevant chunks.
                </li>
                </ul>
			{/if}
			{#if stepNo == 7}
            <ul class="list-disc pl-5">
                <li>
                    Use a generative model to generate a response to the original question, passing in relevant chunks, metadata, and the question itself.
                </li>
                <li>
                    Climate-RAG supports various commercial and open source LLMs.
                </li>
                <li>
                    Modular design using <a href="https://langchain.com" target="_blank">Langchain</a> allows for quick adoption of best available models.
                </li>

                </ul>
			{/if}
			{#if stepNo == 8}
            <ul class="list-disc pl-5">
                <li>
                    If the user is satisfied with the answer, ðŸŽ‰.
                </li>
                <li>
                    If not, find more relevant documents through web search and repeat the process.
                </li>
                </ul>
			{/if}
			{#if stepNo == 9}
                    <ul class="list-none pl-5">
						Answers from this RAG pipeline have a few benefits:
									<ul class="list-disc pl-5">
										<li>
						The LLM can generate citations for the answer, often pointing to the relevant page in PDFs.
										</li>
										<li>
						The LLM is much less likely to hallucinate than something like ChatGPT.
										</li>
										<li>
						Subsequent questions on the same document can be answered more quickly.
										</li>
										<li>
											Complex, multi-step questions can be researched using the CLI tools.
											</li>
									</ul>
					</ul>
			{/if}
		</div>
	</div>
    </div>
{/if}
{#each { length: 9 } as _, i}
	<Transition
		do={() => {
			stepNo = i + 1
		}}
		undo={() => {
			stepNo = i
		}}
	/>
{/each}

<style>
    div {
        transition: all 0.5s ease;
    }
</style>